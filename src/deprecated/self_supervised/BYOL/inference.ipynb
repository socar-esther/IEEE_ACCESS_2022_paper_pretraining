{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset은 labedled로 가져감\n",
    "def create_self_supervised_set(base_dir, target_data_name, self_train_num_per_class, self_test_num_per_class):\n",
    "    ## 하나의 파일로 합치는 작업\n",
    "#     dataroot = '../../../../dataset'\n",
    "    dataroot = os.path.join(base_dir, 'train_size_500', 'train',)\n",
    "    target_names = os.listdir(dataroot)\n",
    "    print('>> check class : ', os.listdir(dataroot))\n",
    "    \n",
    "    file_train_names_list = list()\n",
    "    file_test_names_list_ray = list()\n",
    "    file_test_names_list_stomic = list()\n",
    "\n",
    "    \n",
    "    for target_name in target_names : \n",
    "        \n",
    "        target_path = os.path.join(dataroot, target_name)\n",
    "        \n",
    "        target_train_filenames = sorted(os.listdir(target_path))[:self_train_num_per_class]\n",
    "        target_test_filenames = sorted(os.listdir(target_path))[self_train_num_per_class:self_train_num_per_class+self_test_num_per_class]\n",
    "        \n",
    "        for target_filename in target_train_filenames : \n",
    "            target_filepath = os.path.join(target_path, target_filename)\n",
    "            file_train_names_list.append(target_filepath)\n",
    "        print(target_test_filenames)\n",
    "        for test_filename in target_test_filenames : \n",
    "            if target_name == 'ray' :\n",
    "                target_filepath = os.path.join(target_path, target_filename)\n",
    "                file_test_names_list_ray.append(target_filepath)\n",
    "            elif target_name == 'stonic' :\n",
    "                target_filepath = os.path.join(target_path, target_filename)\n",
    "                file_test_names_list_stomic.append(target_filepath)\n",
    "\n",
    "    # 저장할 파일 생성\n",
    "    save_base = os.path.join(base_dir, target_data_name, 'train_size_' + str(self_train_num_per_class))\n",
    "    train_save_dir_out = os.path.join(save_base, 'train/')\n",
    "    train_save_dir = os.path.join(save_base, 'train/images/')\n",
    "    \n",
    "    if not os.path.exists(train_save_dir) :\n",
    "        os.makedirs(train_save_dir)\n",
    "    # train 파일 저장\n",
    "    for src_filename in file_train_names_list:\n",
    "        img = cv2.imread(src_filename)\n",
    "        cv2.imwrite(os.path.join(train_save_dir, src_filename.split('/')[-1]), img)\n",
    "\n",
    "    \n",
    "    # 저장할 파일 생성\n",
    "    test_save_dir_out = os.path.join(save_base, 'test/')\n",
    "    test_save_dir_ray = os.path.join(save_base, 'test/ray/')\n",
    "    test_save_dir_stonic = os.path.join(save_base, 'test/stonic')\n",
    "    \n",
    "    if not os.path.exists(test_save_dir_ray) :\n",
    "        os.makedirs(test_save_dir_ray)\n",
    "    if not os.path.exists(test_save_dir_stonic) :\n",
    "        os.makedirs(test_save_dir_stonic)\n",
    "        \n",
    "    # test 파일 저장\n",
    "    print(file_test_names_list_ray)\n",
    "    for src_filename in file_test_names_list_ray:\n",
    "        img = cv2.imread(src_filename)\n",
    "        cv2.imwrite(os.path.join(test_save_dir_ray, src_filename.split('/')[-1]), img)\n",
    "        \n",
    "    for src_filename in file_test_names_list_stomic:\n",
    "        img = cv2.imread(src_filename)\n",
    "        cv2.imwrite(os.path.join(test_save_dir_stonic, src_filename.split('/')[-1]), img)\n",
    "    \n",
    "    return train_save_dir_out, test_save_dir_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, net, device):\n",
    "\n",
    "    net.eval()\n",
    "    test_losses = list()\n",
    "    test_trues = list()\n",
    "    test_preds = list()\n",
    "\n",
    "    for idx, (img, label) in enumerate(dataloader):\n",
    "\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        out = net(img)\n",
    "        \n",
    "        softmax_out = torch.softmax(out, 1)\n",
    "        softmax_max = softmax_out.detach().cpu().numpy().max()\n",
    "\n",
    "        _, pred = torch.max(out, 1) # make prediction\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_trues.extend(label.view(-1).cpu().numpy().tolist())\n",
    "        test_preds.extend(pred.view(-1).cpu().detach().numpy().tolist())\n",
    "\n",
    "\n",
    "    acc, f1, auc, prec, rec = calculate_metrics(test_trues, test_preds)\n",
    "    print(confusion_matrix(test_trues, test_preds))\n",
    "\n",
    "    return net, acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(trues, preds):\n",
    "    accuracy = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='macro')\n",
    "    precision = precision_score(trues, preds, average='macro')\n",
    "    recall = recall_score(trues, preds, average='macro')\n",
    "\n",
    "    return accuracy, f1, auc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> check class :  ['images']\n",
      "['20180617_11474_13898837_d1d72a2d3b85eb550ff80c93c5e4be6f.jpg', '20180617_11474_13898839_edc983ccd9ea690954629dfb5535d1a8.jpg', '20180617_11474_13898840_0932b489386a5f6239e11cf7531b75d3.jpg', '20180617_11474_13898841_494d1b820833ea9324d4d225ff5af519.jpg', '20180617_11497_13896910_34ab7ca3a4896c32c34234e64b1f6d2c.jpg', '20180617_11497_13896911_901127ca8615d3c07924fbdf06aad11f.jpg', '20180617_11497_13896912_7d27f1e3651eb9b90f0653174ba323d1.jpg', '20180617_11497_13896913_90076cefc41dce749561e4eb6289c24b.jpg', '20180617_11497_13896914_6d57ce48eef7ce9897bb5a43ef0bb4f3.jpg', '20180617_11497_13896915_d64bc9086594dbcb4e740eae26f024f1.jpg', '20180617_11500_13897203_73f3f32391c1e9adc098f42ac6db26e0.jpeg', '20180617_11500_13897204_406b18aaf3d2a39c3f616de75a24ae10.jpeg', '20180617_11500_13897205_05b98d4f49bda36e479132354bf2ded2.jpeg', '20180617_11500_13897206_a7485514da14cb61783abb6d2d32942a.jpeg', '20180703_1655_14267133_f8837a8a2b533a6faa7ac54e3282a55b.jpg', '20180703_1655_14267134_42cf05b935530054afa011e7979dcf57.jpg', '20180703_1655_14267135_e2436877d78fcea1e05e0b58388c72ef.jpg', '20180703_1655_14267136_d9a0e392dbd49248c8d2f3adfb293bef.jpg', '20180703_1655_14267137_c4b71bff4736185b6477a97651daa369.jpg', '20180703_1658_14272232_164b9d4d012be63bc06b17809c39e815.jpg', '20180703_1658_14272233_2625a4b378bf45ee80d45fbaeca0d08e.jpg', '20180703_1658_14272234_435cc09ab1e5e20e0588c23b10218017.jpg', '20180703_1658_14272235_56e991cf85a1546dccdb9a5f7a0ca3b2.jpg', '20180703_1658_14272236_3f2c56e16e8434946f3b06ee7d1bf8a2.jpg', '20180703_1658_14272237_799bd2f546258a90fdac4410c17139ac.jpg', '20180703_1664_14273092_038d2a7333e059028aa8fb2fb4c46b73.jpeg', '20180703_1664_14273093_e088135c181af312401e13f7d52020d9.jpeg', '20180703_1665_14259517_9390aa2f87fdca9c508cf73b69d6799e.jpg', '20180703_1665_14259518_1684ec0d34610003c26acbeaf7cabe87.jpg', '20180703_1665_14259519_c38a9c97fb9fc5efa0761b09bb799251.jpg', '20180703_1665_14259520_0ec939a01bbdbb78ec36e40bffb61123.jpg', '20180703_1666_14274267_908cf8ba17af4f221bdd6b7846e0f6c5.jpg', '20180703_1672_14260893_cbc50f235734ba2588e2d1b648cff6eb.jpg', '20180703_1672_14260894_fe677483ec9c2f7ef2a31c4de90dc861.jpg', '20180703_1672_14260896_b161aa858032b0b582b0992018be03cb.jpg', '20180703_1672_14260899_1e6ff6bd82cc54391d1ee87e368b1aba.jpg', '20180703_1672_14260900_67d064a8ddfd71398339779b792ffc7a.jpg', '20180703_1672_14260902_8c941bd91b5e5a3b3b7e3f5854ad0daf.jpg', '20180703_1672_14260904_0dbf6e486bf97e136df119354a641868.jpg', '20180703_1672_14272701_07c37ac815221e88710e83ea45f55a4e.jpeg', '20180703_1672_14272702_dc35e53ec268a57dd9df7efb1f270afa.jpeg', '20180703_1694_14260733_05b88756a10cff2c0acd14772654a890.jpeg', '20180703_1694_14264314_f58578971718b91380e3cf8b558c6b49.jpeg', '20180703_1694_14264315_bb3375edb76d953b8e82a718869b314a.jpeg', '20180703_1694_14264316_9f2d1adb4298c1609a9a8199abb31973.jpeg', '20180703_1694_14264317_ee3cab241505183fef1c92889dc809f2.jpeg', '20180703_1694_14264318_04fa9a2c0fd16aeb7a2c51004785b550.jpeg', '20180703_1694_14264319_6e3decf5e70666a284e6eb7c678af912.jpeg', '20180703_1694_14264321_d18ad0b5de99703a7bde363e1302e179.jpeg', '20180703_1694_14264322_f30da45f1ad4c54fe890481e65a9e756.jpeg', '20180703_1703_14261708_6e6ce34eb77b5f8771a5c37003752792.jpeg', '20180703_1703_14261709_a6ff078842ddd3d3a36ca4d5c6aa3ee6.jpeg', '20180703_1703_14261713_3499ef9ed8226157b507feb1fa732fe6.jpeg', '20180703_1705_14267311_0856ed6bb619455810978fd4224b56e6.jpeg', '20180703_1705_14267314_6ab8fac4bc17bae7023ac48d253111fe.jpeg', '20180703_1705_14267315_27c871f789105187f28e59aa45aa007d.jpeg', '20180703_1705_14267316_a1024be372691ba761deb2a7fa74acf0.jpeg', '20180703_1705_14267317_7ca2a066dad54131593b0b02cdc45b51.jpeg', '20180703_1705_14267359_779a4f0415131d8eea9cd63fcd12e3fa.jpeg', '20180703_1705_14267360_6a830e673376d75806416df04a329523.jpeg', '20180703_1723_14258958_f1d56ebb9ec67361c21c506cb04dea49.jpeg', '20180703_1723_14258961_0d6f475e02f8feec2f58bf70226487c9.jpeg', '20180703_1723_14258966_68c37793fdc7b3396807e192a2721906.jpeg', '20180703_1723_14258969_90c8b98562cfb841fca5534f1ed76e38.jpeg', '20180703_1723_14258970_d48a67b9bf4eb3e8a52db78d0c4645be.jpeg', '20180703_1723_14258971_c7376d4c26d0b7f502154850aa76e42e.jpeg', '20180703_1724_14260088_5125f9f296da201eb32b8c39019d4aeb.jpeg', '20180703_1724_14260089_bdc5b7b3f2501430c0b74fa2489b7fcc.jpeg', '20180703_1724_14260090_570fa4eb8aa973ef7e8315665abd2948.jpeg', '20180703_1724_14260091_dee4d6c23ca674002bd2f33aeb95ffa5.jpeg', '20180703_1734_14274328_dc842a9483d5f9678508e140b2124c4c.jpeg', '20180703_1734_14274329_2d18f2379c8eedebc83ffa335a25110c.jpeg', '20180703_1734_14274330_e1a4642e806c097e46ee75262f495fbb.jpeg', '20180703_1734_14274331_17c3d1eaefadd19ff86d7f94f267347c.jpeg', '20180703_1734_14274332_5640048918e6ad67b3aa7c90064b1d1d.jpeg', '20180703_1734_14274333_b24e787a88ce7f694f36db939804c219.jpeg', '20180703_1734_14274334_4fa3b3c6aa119c718ef44cb565182a94.jpeg', '20180703_1734_14274335_fa308d3e2e8c219ee913748c829b5171.jpeg', '20180703_1734_14274336_d8a3d4a1ea8aa89cc5192dbb6eb736ea.jpeg', '20180703_1734_14274337_8e3a1b7413c425ed92dfe19e0815f51c.jpeg', '20180703_1735_14261459_78b118324e1baad3d7c70cd160227239.jpg', '20180703_1735_14261461_1e3fbc5f8124ccb385b84ba7eeee5c9f.jpg', '20180703_1735_14261462_7e44fbdde3aa8cb98888f008c487e308.jpg', '20180703_1735_14261463_72c4d4a04052bd199e87c99ca435028d.jpg', '20180703_1735_14261464_f640a2d9394d13f2d3b85af8ee786e13.jpg', '20180703_1735_14261465_f39941267242fc3e4b979ae122bfb8cc.jpg', '20180703_1735_14261467_dbd2c48a294a1753a1b1588c97d0cbb6.jpg', '20180703_1735_14269256_d43d6d344366033f4c7c0064fe6665ec.jpg', '20180703_1748_14272911_f6622380b182f5f493f120b0f059ce1c.jpg', '20180703_1748_14272912_52ca2188137e1689d72ee1ef11c2917b.jpg', '20180703_1748_14272913_e78edad6972e35e0b6301feb1bffd244.jpg', '20180703_1748_14272914_2269f7d607442d87b9c09b3fd15389fb.jpg', '20180703_1751_14269520_cd94c3c6d161c21eaecebc22ac9f2206.jpg', '20180703_1751_14269521_436f750aaeea36d8d03a7263584354ea.jpg', '20180703_1751_14269522_5031fde8f8a9e666e9b5b6686da7bc20.jpg', '20180703_1752_14262481_4c89d3ff1a4c595dfed2cf842c2983ad.jpg', '20180703_1752_14262483_9d4acefe3097f0c75dd5168d755ca77b.jpg', '20180703_1752_14262485_1ac15b0e9cab06a741d922fb024788c4.jpg', '20180703_1757_14269360_1cfb6bfc44c370a9227e07b37e281bbb.jpg', '20180703_1757_14269361_8a4075297468afb6aa3941e15edd5e8b.jpg']\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./dataset/temp_ray_v_stonic/temp_ray_v_stonic/train_size_500/test/\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7835ef510251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_self_supervised_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dataset/temp_ray_v_stonic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'temp_ray_v_stonic'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself_train_num_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_test_num_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/accida/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/accida/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Supported extensions are: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./dataset/temp_ray_v_stonic/temp_ray_v_stonic/train_size_500/test/\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import yaml\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import *\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "## testset : 클래스별 폴더로 다시 나눔\n",
    "batch_size = 512\n",
    "data_transforms = torchvision.transforms.Compose([transforms.Resize((256, 256)), \n",
    "                                                  transforms.ToTensor()])\n",
    "\n",
    "# dataset_path = '../datasets/carclass_binary/ray_stonic/train_size_500/test'\n",
    "dataset_path = './dataset/temp_ray_v_stonic/train_size_500/test'\n",
    "\n",
    "train_dir, test_dir = create_self_supervised_set(base_dir = './dataset/temp_ray_v_stonic', target_data_name = 'temp_ray_v_stonic' , self_train_num_per_class = 500, self_test_num_per_class = 100)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform = data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,num_workers=0, drop_last=False, shuffle=True)\n",
    "\n",
    "## Resnet 불러와서 model param\n",
    "net = models.resnet50(pretrained=True)\n",
    "best_checkpoint = '../byol_model.pt'\n",
    "net.load_state_dict(torch.load(best_checkpoint, map_location=device))\n",
    "net.to(device)\n",
    "\n",
    "## 결과 확인\n",
    "net, test_acc, test_prec, test_rec, test_f1 = test(test_loader, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m68"
  },
  "kernelspec": {
   "display_name": "accida",
   "language": "python",
   "name": "accida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
